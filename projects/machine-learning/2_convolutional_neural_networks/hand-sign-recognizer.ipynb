{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hand-sign recognizer\n",
    "Here we build a convolutional neural-network for recognizing hand-signs.\n",
    "\n",
    "> Inspired by: [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available.\n",
      "Setting torch.cuda.DoubleTensor as default dtype...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import util\n",
    "\n",
    "import importlib\n",
    "importlib.reload(util)\n",
    "\n",
    "if torch.cuda.is_available(): # TODO: remove the false\n",
    "    print(\"Cuda available.\")\n",
    "    tensor_type = 'torch.cuda.DoubleTensor'\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "else:\n",
    "    print(\"Cuda not found.\")\n",
    "    tensor_type = 'torch.DoubleTensor'\n",
    "\n",
    "print(f\"Setting {tensor_type} as default dtype...\")\n",
    "torch.set_default_tensor_type(tensor_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training-examples: 1080\n",
      "# of test-examples: 120\n",
      "image-dimensions: (64, 64, 3)\n",
      "class-labels: [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y, class_labels = None, None, None, None, None\n",
    "with h5py.File(\"../data/test-hand-signs.h5\", \"r\") as f:\n",
    "    test_x = np.array(f[\"test_set_x\"])\n",
    "    test_y = np.array(f[\"test_set_y\"])\n",
    "\n",
    "with h5py.File(\"../data/train-hand-signs.h5\", \"r\") as f:\n",
    "    train_x = np.array(f[\"train_set_x\"])\n",
    "    train_y = np.array(f[\"train_set_y\"])\n",
    "    class_labels = np.array(f[\"list_classes\"])\n",
    "\n",
    "print(f\"# of training-examples: {train_x.shape[0]}\")\n",
    "print(f\"# of test-examples: {test_x.shape[0]}\")\n",
    "print(f\"image-dimensions: {test_x.shape[1:]}\")\n",
    "print(f\"class-labels: {class_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # pre-processing\n",
    "We will perform mean and variance normalization of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set X: (1080, 64, 64, 3), Y: (1080, 6)\n",
      "Test-set X: (120, 64, 64, 3), Y: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "train_mean = np.mean(a=train_x, axis=0)\n",
    "train_std = np.std(a=train_x, axis=0)\n",
    "\n",
    "# train-set normalization\n",
    "train_X = (train_x - train_mean) / train_std\n",
    "train_Y = np.eye(len(class_labels))[train_y, :].copy()\n",
    "\n",
    "# test-set normalization\n",
    "test_X = (test_x - train_mean) / train_std\n",
    "test_Y = np.eye(len(class_labels))[test_y, :].copy()\n",
    "\n",
    "print(f\"Train-set X: {train_X.shape}, Y: {train_Y.shape}\")\n",
    "print(f\"Test-set X: {test_X.shape}, Y: {test_Y.shape}\")\n",
    "\n",
    "cu_train_X = torch.tensor(train_X)\n",
    "cu_train_Y = torch.tensor(train_Y)\n",
    "cu_test_X = torch.tensor(test_X)\n",
    "cu_test_Y = torch.tensor(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # architecture\n",
    "We will use a 3-layer CNN, as defined below:\n",
    "\n",
    "<center>\n",
    "\n",
    "| #-layer | layer-type      | component        | properties                           |         |\n",
    "|---------|-----------------|------------------|--------------------------------------|---------|\n",
    "| 1       | 2d-convolution  | kernel           | $(h^{[1]}_k, w^{[1]}_k)$             | $(4,4)$ |\n",
    "|         |                 | #-kernels        | $c^{[1]}$                            | 8       |\n",
    "|         |                 | convolve-pding | $(h^{[1]}_p, w^{[1]}_p)$             |`<same>` |\n",
    "|         |                 | convolve-stride  | $(h^{[1]}_s, w^{[1]}_s)$             | $(1,1)$ |\n",
    "|         |                 | pooling          | max-pooling                          |         |\n",
    "|         |                 | pooling-filter   | $(h^{[1]}_l, w^{[1]}_l)$             | $(8,8)$ |\n",
    "|         |                 | pooling-padding  | $({}^{l}h^{[1]}_p, {}^{l}w^{[1]}_p)$ |`<same>` |\n",
    "|         |                 | pooling-stride   | $({}^{l}h^{[1]}_s, {}^{l}w^{[1]}_s)$ | $(8,8)$ |\n",
    "|         |                 | activation       | ReLU                                 |         |\n",
    "| 2       | 2d-convolution  | kernel           | $(h^{[2]}_k, w^{[2]}_k)$             | $(2,2)$ |\n",
    "|         |                 | #-kernels        | $c^{[2]}$                            | 16      |\n",
    "|         |                 | convolve-padding | $(h^{[2]}_p, w^{[2]}_p)$             |`<same>` |\n",
    "|         |                 | convolve-stride  | $(h^{[2]}_s, w^{[2]}_s)$             | $(1,1)$ |\n",
    "|         |                 | pooling          | max-pooling                          |         |\n",
    "|         |                 | pooling-filter   | $(h^{[2]}_l, w^{[2]}_l)$             | $(4,4)$ |\n",
    "|         |                 | pooling-padding  | $({}^{l}h^{[2]}_p, {}^{l}w^{[2]}_p)$ |`<same>` |\n",
    "|         |                 | pooling-stride   | $({}^{l}h^{[2]}_s, {}^{l}w^{[2]}_s)$ | $(4,4)$ |\n",
    "|         |                 | activation       | ReLU                                 |         |\n",
    "| 3       | fully-connected | #-neurons        | $n^{[3]}$                            | $6$     |\n",
    "|         |                 | activation       | softmax                              |         |\n",
    "\n",
    "</center>\n",
    "\n",
    "Also, the number of channels in the input, i.e. $c^{[0]} = 3$.\n",
    "\n",
    "* Given the stride and kernel, `<same>` padding refers to the padding regime wherein the output immediately after the convolution is of the same size as that of the input, i.e.\n",
    "$$\n",
    "h^{[l]}_z = \\left\\lfloor\\frac{h^{[l]}_a + 2h^{[l]}_p - h^{[l]}_k}{h^{[l]}_s} + 1\\right\\rfloor = h^{[l]}_a;\\qquad w^{[l]}_z = \\left\\lfloor\\frac{w^{[l]}_a + 2w^{[l]}_p - w^{[l]}_k}{w^{[l]}_s} + 1\\right\\rfloor = w^{[l]}_a\n",
    "$$\n",
    "\n",
    "## # forward-propagation\n",
    "> **Note**: for the forward-propagation equations, read [Section-2, Back-propagation: Conv2D](./back-propagation_Conv2D.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogate(X: torch.Tensor, model_arch: dict, model: dict, **kwargs) -> tuple:\n",
    "    '''\n",
    "    X: shape -> (m, h_a^{0}, w_a^{0}, c^{0})\n",
    "    model_arch: dictionary of attributes that define the model architecture\n",
    "    model: dictionary containing the model parameters (i.e. kernels, weights, and biases)\n",
    "    '''\n",
    "\n",
    "    L: int = model_arch[\"L\"]\n",
    "\n",
    "    cache = {'m': X.shape[0], 'c-l0': (None, None, None, None, None, None, torch.clone(X).detach())}\n",
    "\n",
    "    m = X.shape[0]\n",
    "    Al_1 = X\n",
    "    Al = None\n",
    "    for l in np.arange(start=1, stop=L+1):\n",
    "        if model_arch[\"t-l\" + str(l)] == \"conv\":\n",
    "            cl = model_arch[\"c-l\" + str(l)]\n",
    "\n",
    "            Kl = torch.clone(model[\"K-l\" + str(l)]).detach() # shape -> (c^{l}, h_k^{l}, w_k^{l}, c^{l-1})\n",
    "            bl = torch.clone(model['b-l' + str(l)]).detach() # shape -> (c^{l})\n",
    "\n",
    "            # shape -> (h_k^{l} * w_k^{l} * c^{l-1}, c^{l})\n",
    "            Kl_v = torch.transpose(torch.reshape(torch.permute(\n",
    "                Kl, dims=(0,3,2,1)), shape=(cl,-1)), dim0=1, dim1=0)\n",
    "\n",
    "            p = model_arch[\"p-l\" + str(l)]\n",
    "            # shape -> (m, h_a^{l-1} + 2 * h_p^{l}, w_a^{l-1} + 2 * w_p^{l}, c^{l-1} )\n",
    "            Al_1 = torch.nn.functional.pad(torch.clone(Al_1).detach(), \n",
    "                pad=(p[-1], p[-1], p[-2], p[-2], p[-3], p[-3], 0, 0), # 2d-Conv\n",
    "                mode=\"constant\", value=0.0)\n",
    "            # shape -> (m, h_i^{l}, w_i^{l}, h_k^{l}, w_k^{l}, c^{l-1})\n",
    "            win_Al_1 = torch.squeeze(util.view_as_window(\n",
    "                arr_in=Al_1, window_shape=(1, *model_arch[\"k-l\" + str(l)]),\n",
    "                step=(1, *model_arch[\"s-l\" + str(l)])))\n",
    "            i = model_arch[\"i-l\" + str(l)]\n",
    "            # shape -> (m, h_i^{l} * w_i^{l}, h_k^{l} * w_k^{l} * c^{l-1})\n",
    "            win_Al_1_v = torch.reshape(\n",
    "                torch.permute(win_Al_1, dims=(0,2,1,5,4,3)), \n",
    "                shape=(m, i[0] * i[1], Kl_v.shape[0]))\n",
    "            \n",
    "            # shape -> (m, h_i^{l} * w_i^{l}, c^{l})\n",
    "            Zl_v = torch.matmul(win_Al_1_v, Kl_v) \\\n",
    "                + torch.reshape(bl, shape=(1,1,-1)) # shape -> (1,1,c^{l})\n",
    "            # the following step is only suited for element-wise activation, \n",
    "            # since we are not specifying any axis along which to normalize\n",
    "            Il_v = util.activation(Zl_v, model_arch[\"g-l\" + str(l)])\n",
    "\n",
    "            # shape -> (m, h_i^{l}, w_i^{l}, c^{l})\n",
    "            Il = torch.permute(torch.reshape(torch.transpose(\n",
    "                Il_v, dim0=1,dim1=2), shape=(m, cl, i[1], i[0])), dims=(0,3,2,1))\n",
    "\n",
    "            p = model_arch[\"pl-p-l\" + str(l)]\n",
    "            # shape -> (m, h_i^{l} + 2 * l_h_p^{l}, w_i^{l} + 2 * l_w_p^{l}, c^{l})\n",
    "            Il = torch.nn.functional.pad(torch.clone(Il).detach(), \n",
    "                pad=(p[-1], p[-1], p[-2], p[-2], p[-3], p[-3], 0, 0),\n",
    "                mode=\"constant\", value=0.0)\n",
    "            # shape -> (m, h_a^{l}, w_a^{l}, c^{l}, l_h_k^{l}, l_w_k^{l})\n",
    "            win_Il = torch.squeeze(util.view_as_window(\n",
    "                arr_in=Il,\n",
    "                window_shape=(1,*model_arch[\"pl-k-l\" + str(l)]),\n",
    "                step=(1, *model_arch[\"pl-s-l\" + str(l)])))\n",
    "\n",
    "            a = model_arch[\"a-l\" + str(l)]\n",
    "            # shape -> (m, c^{l}, h_a^{l} * w_a^{l}, l_h_k^{l} * l_w_k^{l})\n",
    "            win_Il_v = torch.reshape(torch.permute(\n",
    "                win_Il, dims=(0,3,2,1,5,4)), \n",
    "                shape=(m, cl, a[0] * a[1], math.prod(model_arch[\"pl-k-l\" + str(l)])))\n",
    "            max_pool = torch.max(win_Il_v, dim=len(win_Il_v.shape) - 1, keepdim=False)\n",
    "\n",
    "            # shape -> (m, h_a^{l}, w_a^{l}, c^{l})\n",
    "            Al = torch.permute(torch.reshape(\n",
    "                max_pool.values, shape=(m, cl, a[1], a[0])), dims=(0, 3, 2, 1))\n",
    "            # shape -> (m, h_a^{l}, w_a^{l}, c^{l})\n",
    "            max_pool_switches = torch.permute(torch.reshape(\n",
    "                max_pool.indices, shape=(m, cl, a[1], a[0])), dims=(0, 3, 2, 1))\n",
    "\n",
    "            cache['c-l' + str(l + 1)] = (\n",
    "                model_arch[\"t-l\" + str(l)],\n",
    "                torch.clone(Kl).detach(), \n",
    "                torch.clone(bl).detach(),\n",
    "                # shape -> (m, h_i^{l}, w_i^{l}, c^{l})\n",
    "                torch.permute(torch.reshape(torch.transpose(\n",
    "                    Zl_v, dim0=1,dim1=2), shape=(m, cl, i[1], i[0])), dims=(0,3,2,1)), \n",
    "                torch.clone(Il).detach(), \n",
    "                max_pool_switches,\n",
    "                torch.clone(Al).detach()\n",
    "            )\n",
    "\n",
    "        elif model_arch[\"t-l\" + str(l)] == \"flat\":\n",
    "            # shape -> (m, h_a * w_a * c^{l-1})\n",
    "            Al_1_v = torch.reshape(torch.permute(\n",
    "                Al_1, dims=(0,3,2,1)), shape=(m, -1))\n",
    "\n",
    "            Wl = model[\"W-l\" + str(l)]\n",
    "            bl = model[\"b-l\" + str(l)]\n",
    "\n",
    "            # shape -> (m, n^{l})\n",
    "            Zl = util.linear(Wl, Al_1_v, bl)\n",
    "\n",
    "            # shape -> (m, n^{l})\n",
    "            Al = util.activation(Zl, model_arch[\"g-l\" + str(l)])\n",
    "\n",
    "            cache['c-l' + str(l + 1)] = (\n",
    "                model_arch[\"t-l\" + str(l)], \n",
    "                torch.clone(Al).detach(),\n",
    "                torch.clone(Wl).detach(), \n",
    "                torch.clone(bl).detach(),\n",
    "                torch.clone(Zl).detach()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid layer-type: {model_arch['t-l' + str(l)]} at {'t-l' + str(l)}\")\n",
    "\n",
    "        Al_1 = Al\n",
    "\n",
    "    return Al, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # backward-propagation\n",
    "> **Note**: for the backward-propagation diagram, equations, and their derivation, read [Section-3.1, Back-propagation: Conv2D](./back-propagation_Conv2D.pdf).\n",
    ">\n",
    ">For the derivation of $\\frac{\\mathrm{d}J}{\\mathrm{d}\\mathbf{A}^{[3]}}$, and the derivation of $\\frac{\\mathrm{d}\\mathbf{A}^{[3]}}{\\mathrm{d}\\mathbf{Z}^{[3]}}$, when $\\mathbf{A}^{[3]} = \\mathrm{softmax}(\\mathbf{Z}^{[3]})$; see the section titled `backward-propagation` in [`..\\1_multi_layer_perceptrons\\hand-sign-recognizer.ipynb`](../1_multi_layer_perceptrons/hand-sign-recognizer.ipynb)\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_backprop(l: int, pool_type: str, max_pool_switches: torch.Tensor, model_arch: dict) -> torch.Tensor:\n",
    "    '''\n",
    "    computes the derivative dA/dI, where I --(pooling)--> A.\n",
    "\n",
    "    l: the current layer number\n",
    "    pool_type: type of pooling used. Could be 'max' or 'avg'\n",
    "    max_pool_switches: shape -> (m, h_a^{l}, w_a^{l}, c^{l})\n",
    "    model_arch: dictionary defining the model architecture. This the same \n",
    "        dictionary that is passed into the initialize() method.\n",
    "\n",
    "    dA_dI: shape -> (m, c^{l}, h_a^{l} * w_a^{l}, h_i^{l} * w_i^{l})\n",
    "    '''\n",
    "\n",
    "    dA_dI = None\n",
    "    m, h_a, w_a, cl = max_pool_switches.shape\n",
    "\n",
    "    if pool_type == 'max':\n",
    "        h_i, w_i, _ = model_arch[\"i-l\" + str(l)]\n",
    "        h_a, w_a, _ = model_arch[\"a-l\" + str(l)]\n",
    "        h_l, w_l, _ = model_arch[\"pl-k-l\" + str(l)]\n",
    "        l_h_s, l_w_s, _ = model_arch[\"pl-s-l\" + str(l)]\n",
    "        l_h_p, l_w_p, _ = model_arch[\"pl-p-l\" + str(l)]\n",
    "\n",
    "        max_pool_switches += 1 # transform from 0-indexed to 1-indexed\n",
    "        c_b = torch.ceil(max_pool_switches / h_l)\n",
    "        r_b = max_pool_switches - (c_b - 1) * h_l\n",
    "\n",
    "        r_a = torch.reshape(torch.arange(\n",
    "            start=1, end=h_a + 1, step=1), shape=(1, h_a, 1, 1))\n",
    "        c_a = torch.reshape(torch.arange(\n",
    "            start=1, end=w_a + 1, step=1), shape=(1, 1, w_a, 1))\n",
    "\n",
    "        r_t = 1 + l_h_s * (r_a - 1) - l_h_p\n",
    "        c_t = 1 + l_w_s * (c_a - 1) - l_w_p\n",
    "\n",
    "        r_i = r_b + r_t - 1\n",
    "        c_i = c_b + c_t - 1\n",
    "\n",
    "        # shape -> max_pool_switches.shape\n",
    "        j = (c_i - 1) * h_i + r_i - 1 # -1 for zero-indexing\n",
    "        # shape -> (m, c^{l}, h_a^{l} * w_a^{l})\n",
    "        j = torch.reshape(torch.permute(\n",
    "            j, dims=(0, 3, 2, 1)), shape=(m, cl, h_a * w_a, 1))\n",
    "        \n",
    "        # shape -> (m, c^{l}, h_a^{l} * w_a^{l}, h_i^{l} * w_i^{l})\n",
    "        dA_dI = torch.zeros(size=(m, cl, h_a * w_a, h_i * w_i))\n",
    "        source_ones = torch.ones_like(j) * 1.0 # * 1.0 is to convert dtype to float\n",
    "        dA_dI = torch.scatter(input=dA_dI, dim=len(dA_dI.shape) - 1, index=j, src=source_ones)\n",
    "    elif pool_type == 'avg':\n",
    "        # TODO: implement this\n",
    "        dA_dI = None\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid pooling-type: {pool_type}\")\n",
    "    \n",
    "    return dA_dI\n",
    "\n",
    "def gradients(Y: torch.Tensor, forward_cache: dict, model: dict, model_arch: dict, **kwargs):\n",
    "    '''Computes gradients for backward propagation.\n",
    "\n",
    "    Y: shape -> (m, nL)\n",
    "    forward_cache: the cache of matrices returned by the forward_propogate() method\n",
    "    model: dictionary containing the model parameters (i.e. kernels, weights, and biases) \n",
    "    model_arch: dictionary defining the model architecture. This the same \n",
    "        dictionary that is passed into the initialize() method.\n",
    "    '''\n",
    "\n",
    "    backward_cache = dict()\n",
    "    m = forward_cache['m']\n",
    "    L = model[\"L\"]\n",
    "\n",
    "    dAl, dWl, dbl, dZl = None, None, None, None\n",
    "    Al = forward_cache['c-l' + str(L)][1]\n",
    "\n",
    "    _m, nL = Al.shape\n",
    "    assert _m == m, f\"batch-sizes {m} != {_m}\"\n",
    "\n",
    "    Al_3d = Al.reshape((m, nL, 1))\n",
    "    dAl = torch.divide(- Y.reshape((m, nL, 1)), m * Al_3d)\n",
    "    for l in np.arange(start=L, stop=0, step=-1):\n",
    "        if model_arch['t-l' + str(l)] == 'flat':\n",
    "            _, Al, Wl, bl, Zl = forward_cache['c-l' + str(l)]\n",
    "\n",
    "            if model_arch['g-l' + str(l)] == 'softmax':\n",
    "                # shape -> (m, nL)\n",
    "                dZl = torch.multiply(dAl , \n",
    "                    torch.multiply(torch.eye(nL).reshape((1, nL, nL)), Al_3d) \\\n",
    "                    - torch.multiply(Al_3d, torch.transpose(Al_3d, dim0=1, dim1=2))) \\\n",
    "                    .sum(dim=1, keepdim=False)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown activation-function {model['g-l' + str(l)]} for {'g-l' + str(l)}\")\n",
    "\n",
    "            Al_1 = forward_cache['c-l' + str(l-1)][0]\n",
    "            dWl = torch.matmul(dZl.T, Al_1.T) # .T to get gradients\n",
    "\n",
    "            dbl = torch.matmul(torch.ones((1, m)), dZl) # .T to get gradients\n",
    "            backward_cache[\"dc-l\" + str(l)] = (dAl, dZl, dWl, dbl)\n",
    "\n",
    "            dAl_1 = torch.matmul(dZl, Wl.T) # computes dA^{[l-1]}\n",
    "\n",
    "            dAl = dAl_1\n",
    "        elif model_arch['t-l' + str(l)] == 'conv':\n",
    "            _, Kl, bl, Zl, Il, max_pool_switches, Al = forward_cache['c-l' + str(l)]\n",
    "            cl = model_arch[\"c-l\" + str(l)]\n",
    "            i = model_arch[\"i-l\" + str(l)]\n",
    "            a = model_arch[\"a-l\" + str(l)]\n",
    "\n",
    "            dIl_mask = pooling_backprop(l=l, pool_type=model_arch[\"pl-t-l\" + str(l)], \n",
    "                max_pool_switches=max_pool_switches, model_arch=model_arch)\n",
    "            \n",
    "            if l+1 <= L and model_arch[\"t-l\" + str(l+1)] == 'flat':\n",
    "                # shape -> (m, c^{l}, h_a^{l} * w_a^{l}, 1)\n",
    "                dAl = torch.reshape(dAl, shape=(m, cl, a[0] * a[1], 1))\n",
    "            # shape -> (m, c^{l}, h_i^{l} * w_i^{l})\n",
    "            dIl = torch.squeeze(torch.multiply(dAl, dIl_mask).sum(dim=2))\n",
    "            \n",
    "            # shape -> (m, c^{l}, h_i^{l} * w_i^{l})\n",
    "            dI_dZ = None\n",
    "            if model_arch[\"g-l\" + str(l)] == 'relu':\n",
    "                # shape -> (m, c^{l}, h_i^{l} * w_i^{l})\n",
    "                Zl = torch.reshape(torch.permute(\n",
    "                    Zl, dims=(0,3,2,1)), shape=(m, cl, i[0] * i[1]))\n",
    "                dI_dZ = torch.where(Zl > 0, 1, 0)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid activation function: {model_arch['g-l' + str(l)]}\")\n",
    "            \n",
    "            # shape -> (m, c^{l}, h_i^{l} * w_i^{l})\n",
    "            dZl = torch.multiply(dIl, dI_dZ)\n",
    "\n",
    "            k = model_arch[\"k-l\" + str(l)]\n",
    "            s = model_arch[\"s-l\" + str(l)]\n",
    "            Al_1 = torch.clone(forward_cache[\"c-l\" + str(l-1)][-1]).detach()\n",
    "            cl_1 = model_arch[\"c-l\" + str(l-1)]\n",
    "            # shape -> (m, h_i^{l}, w_i^{l}, c^{l-1}, h_k^{l}, w_k^{l})\n",
    "            Al_1 = torch.squeeze(util.view_as_window(\n",
    "                arr_in=Al_1, window_shape=(1, k[0], k[1], 1), step=(1, s[0], s[1], 1)))\n",
    "            # shape -> (m, c^{l-1}, h_i^{l} * w_i^{l}, h_k^{l} * w_k^{l})\n",
    "            Al_1 = torch.reshape(torch.permute(\n",
    "                Al_1, dims=(0, 3, 2, 1, 5, 4)), shape=(m, cl_1, i[0] * i[1], k[0] * k[1]))\n",
    "            # shape -> (m, c^{l-1}, h_i^{l} * w_i^{l}, h_k^{l} * w_k^{l})\n",
    "            # dZ_dK = torch.matmul(Al_1, torch.eye(n=k[0] * k[1]).reshape((1,k[0],k[1])))\n",
    "            dZ_dK = Al_1\n",
    "            # shape -> (m, c^{l-1}, c^{l}, h_k^{l} * w_k^{l})\n",
    "            dKl = torch.matmul(torch.reshape(\n",
    "                dZl, shape=(m, 1, cl, i[0] * i[1])), dZ_dK)\n",
    "            \n",
    "            # shape -> (m, c^{l})\n",
    "            dbl = torch.sum(dZl, dim=2)\n",
    "\n",
    "            backward_cache[\"dc-l\" + str(l)] = (\n",
    "                # shape -> (m, c^{l}, h_a^{l} * w_a^{l})\n",
    "                torch.clone(torch.squeeze(dAl)).detach(),\n",
    "                # shape -> (m, c^{l}, h_i^{l} * w_i^{l})\n",
    "                torch.clone(dIl).detach(),\n",
    "                # shape -> (m, c^{l}, h_i^{l} * w_i^{l})\n",
    "                torch.clone(dZl).detach(),\n",
    "                torch.clone(\n",
    "                    torch.transpose(torch.reshape(torch.permute(torch.sum(dKl,dim=0), \n",
    "                        # shape -> (c^{l}, c^{l-1}, h_k^{l} * w_k^{l})\n",
    "                        dims=(1,0,2)),\n",
    "                        shape=(cl, cl_1, k[1], k[0])), \n",
    "                        # shape -> (c^{l}, h_k^{l}, w_k^{l}, c^{l-1})\n",
    "                        dim0=1, dim1=3)\n",
    "                ).detach(),\n",
    "                torch.clone(torch.reshape(torch.sum(dbl, dim=0), shape=(cl,1))).detach()\n",
    "            )\n",
    "\n",
    "            al_1 = model_arch[\"a-l\" + str(l-1)]\n",
    "            int_dA = torch.eye(n=al_1[0] * al_1[1])\n",
    "            # shape -> (h_a^{l-1}, w_a^{l-1}, h_a^{l-1} * w_a^{l-1})\n",
    "            int_dA = torch.permute(torch.reshape(\n",
    "                int_dA, shape=(al_1[0] * al_1[1], al_1[1], al_1[0])), dims=(2,1,0))\n",
    "            # shape -> (h_i^[l], w_i^{l}, h_a^{l-1} * w_a^{l-1}, h_k^{l}, w_k^{l})\n",
    "            int_dA = torch.squeeze(util.view_as_window(\n",
    "                arr_in=int_dA, window_shape=(k[0], k[1], 1), step=(s[0], s[1], 1)))\n",
    "            # shape -> (1, h_i^[l] * w_i^{l}, h_a^{l-1} * w_a^{l-1}, h_k^{l} * w_k^{l})\n",
    "            int_dA = torch.reshape(torch.permute(\n",
    "                int_dA, dims=(2,1,0,4,3)), shape=(1, i[0] * i[1], al_1[0] * al_1[1], k[0] * k[1]))\n",
    "            # shape -> (c^{l-1}, 1, h_k^{l} * w_k^{l}, c^{l})\n",
    "            k_vec = torch.transpose(torch.reshape(torch.permute(\n",
    "                Kl, dims=(3,0,2,1)), shape=(cl_1, 1, cl, k[0] * k[1])), dim0=1, dim1=2)\n",
    "            # shape -> (c^{l-1}, h_i^[l] * w_i^{l}, h_a^{l-1} * w_a^{l-1}, c^{l})\n",
    "            #       -> (c^{l-1}, c^{l}, h_i^[l] * w_i^{l}, h_a^{l-1} * w_a^{l-1})\n",
    "            dZ_dA = torch.permute(torch.matmul(\n",
    "                int_dA, k_vec), dims=(0,3,1,2))\n",
    "            # shape -> (m, c^{l-1}, c^{l}, h_i^[l] * w_i^{l}, h_a^{l-1} * w_a^{l-1})\n",
    "            dAl_1 = torch.multiply(\n",
    "                torch.reshape(dZl, shape=((m, 1, cl, i[0] * i[1], 1))),\n",
    "                torch.reshape(dZ_dA, shape=(1, cl_1, cl, i[0] * i[1], al_1[0] * al_1[1])))\n",
    "            # shape -> (m, c^{l-1}, h_a^{l-1} * w_a^{l-1})\n",
    "            dAl_1 = torch.sum(dAl_1, dim=(2,3), keepdim=False)\n",
    "\n",
    "            dAl = dAl_1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid layer-type: {model_arch['t-l' + str(l)]} in cache: {'c-l' + str(l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(model_arch: dict):\n",
    "    L = model_arch[\"L\"]\n",
    "    model = dict()\n",
    "\n",
    "    for l in np.arange(start=1,stop=L+1):\n",
    "        layer_type = model_arch[\"t-l\" + str(l)]\n",
    "        if layer_type == \"conv\":\n",
    "            Kl = torch.nn.init.xavier_normal_(torch.empty(\n",
    "                    size=[model_arch[\"c-l\" + str(l)]] + list(model_arch[\"k-l\" + str(l)])))\n",
    "            bl = torch.zeros(size=(model_arch[\"c-l\" + str(l)],1))\n",
    "            \n",
    "            model_arch[\"p-l\" + str(l)] = util.compute_padding(\n",
    "                p_template=model_arch[\"p-l\" + str(l)],\n",
    "                a_shape=model_arch[\"a-l\" + str(l-1)],\n",
    "                k_shape=model_arch[\"k-l\" + str(l)],\n",
    "                s_shape=model_arch[\"s-l\" + str(l)])\n",
    "            # value -> (h_i^{l}, w_i^{l}, 1)\n",
    "            i_size = list(util.convolved_size(\n",
    "                a_shape=model_arch[\"a-l\" + str(l-1)],\n",
    "                k_shape=model_arch[\"k-l\" + str(l)],\n",
    "                p_shape=model_arch[\"p-l\" + str(l)],\n",
    "                s_shape=model_arch[\"s-l\" + str(l)]))\n",
    "            i_size[-1] = model_arch['c-l' + str(l)]\n",
    "            # value -> (h_i^{l}, w_i^{l}, c^{l})\n",
    "            model_arch[\"i-l\" + str(l)] = tuple(i_size)\n",
    "            \n",
    "            model_arch[\"pl-p-l\" + str(l)] = util.compute_padding(\n",
    "                p_template=model_arch[\"pl-p-l\" + str(l)],\n",
    "                a_shape=model_arch[\"i-l\" + str(l)],\n",
    "                k_shape=model_arch[\"pl-k-l\" + str(l)],\n",
    "                s_shape=model_arch[\"pl-s-l\" + str(l)])\n",
    "            # value -> (h_a^{l}, w_a^{l}, 1)\n",
    "            a_size = list(util.convolved_size(\n",
    "                a_shape=model_arch[\"i-l\" + str(l)],\n",
    "                k_shape=model_arch[\"pl-k-l\" + str(l)],\n",
    "                p_shape=model_arch[\"pl-p-l\" + str(l)],\n",
    "                s_shape=model_arch[\"pl-s-l\" + str(l)]))\n",
    "            a_size[-1] = model_arch[\"c-l\" + str(l)]\n",
    "            # value -> (h_a^{l}, w_a^{l}, c^{l})\n",
    "            model_arch[\"a-l\" + str(l)] = tuple(a_size)\n",
    "\n",
    "            model[\"K-l\" + str(l)] = Kl\n",
    "            model[\"b-l\" + str(l)] = bl\n",
    "        elif layer_type == \"flat\":\n",
    "            Wl = torch.nn.init.xavier_normal_(torch.empty(\n",
    "                size=(math.prod(model_arch[\"a-l\" + str(l-1)]), model_arch[\"n-l\" + str(l)])))\n",
    "            bl = torch.zeros(size=(model_arch[\"n-l\" + str(l)],1))\n",
    "\n",
    "            model[\"W-l\" + str(l)] = Wl\n",
    "            model[\"b-l\" + str(l)] = bl\n",
    "        else:\n",
    "            raise KeyError(f\"Invalid layer-type: {layer_type}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_grad_params = None\n",
    "def update_model(model: dict, iteration: int,\n",
    "                 backward_cache: dict, alpha: float=0.001 , beta1: float=0.9, \n",
    "                 beta2: float=0.999, epsilon: float=1e-08, **kwargs):\n",
    "    \"\"\"Uses the ADAM optimization technique to update the parameters.\n",
    "\n",
    "    model: the dictionary of parameters returned by the call to initialize()\n",
    "    iteration: the current iteration across all epochs and batches\n",
    "    backward_cache: \n",
    "        if model_arch[\"t-l\" + str(l)] == 'flat': (dAl, dZl, dWl, dbl)\n",
    "        if model_arch[\"t-l\" + str(l)] == 'conv': (dAl, dIl, dZl, dKl, dbl)\n",
    "    alpha: the learning rate\n",
    "    beta1: the smoothing constant for momentum\n",
    "    beta2: the smoothing constant for RMS-prop\n",
    "    epsilon: non-zero factor for RMS-prop\n",
    "    \"\"\"\n",
    "\n",
    "    if last_grad_params is None:\n",
    "        last_grad_params = dict()\n",
    "        for l in range(1, model_arch[\"L\"] + 1):\n",
    "            if model_arch[\"t-l\" + str(l)] == \"flat\":\n",
    "                last_grad_params['v-W-l' + str(l)] = torch.zeros_like(model['W-l' + str(l)])\n",
    "                last_grad_params['s-W-l' + str(l)] = torch.zeros_like(model['W-l' + str(l)])\n",
    "            elif model_arch[\"t-l\" + str(l)] == \"conv\":\n",
    "                last_grad_params['v-K-l' + str(l)] = torch.zeros_like(model['K-l' + str(l)])\n",
    "                last_grad_params['s-K-l' + str(l)] = torch.zeros_like(model['K-l' + str(l)])\n",
    "            \n",
    "            last_grad_params['v-b-l' + str(l)] = torch.zeros_like(model['b-l' + str(l)])\n",
    "            last_grad_params['s-b-l' + str(l)] = torch.zeros_like(model['b-l' + str(l)])\n",
    "    \n",
    "    for l in range(1, model_arch[\"L\"] + 1):\n",
    "        if model_arch[\"t-l\" + str(l)] == 'flat':\n",
    "            _, _, dWl, dbl = backward_cache[\"dc-l\" + str(l)]\n",
    "\n",
    "            last_grad_params['v-W-l' + str(l)] = (1-beta1) * dWl.T \\\n",
    "                + beta1*last_grad_params['v-W-l' + str(l)]\n",
    "            last_grad_params['v-b-l' + str(l)] = (1-beta1) * dbl.T \\\n",
    "                + beta1*last_grad_params['v-b-l' + str(l)]\n",
    "\n",
    "            last_grad_params['s-W-l' + str(l)] = (1-beta2) * torch.square(dWl.T) \\\n",
    "                + beta2*last_grad_params['s-W-l' + str(l)]\n",
    "            last_grad_params['s-b-l' + str(l)] = (1-beta2) * torch.square(dbl.T) \\\n",
    "                + beta2*last_grad_params['s-b-l' + str(l)]\n",
    "\n",
    "            model['W-l' + str(l)] += -alpha * torch.divide(\n",
    "                last_grad_params['v-W-l' + str(l)] / (1 - np.power(beta1, iteration)), \n",
    "                torch.sqrt(last_grad_params['s-W-l' + str(l)] / (1 - np.power(beta2, iteration))) \n",
    "                    + epsilon)\n",
    "            model['b-l' + str(l)] += -alpha * torch.divide(\n",
    "                last_grad_params['v-b-l' + str(l)] / (1 - np.power(beta1, iteration)), \n",
    "                torch.sqrt(last_grad_params['s-b-l' + str(l)] / (1 - np.power(beta2, iteration)))\n",
    "                    + epsilon)\n",
    "        elif model_arch[\"t-l\" + str(l)] == 'conv':\n",
    "            _, _, _, dKl, dbl = backward_cache[\"dc-l\" + str(l)]\n",
    "\n",
    "            last_grad_params['v-K-l' + str(l)] = (1-beta1) * dKl \\\n",
    "                + beta1*last_grad_params['v-K-l' + str(l)]\n",
    "            last_grad_params['v-b-l' + str(l)] = (1-beta1) * dbl \\\n",
    "                + beta1*last_grad_params['v-b-l' + str(l)]\n",
    "\n",
    "            last_grad_params['s-K-l' + str(l)] = (1-beta2) * torch.square(dKl) \\\n",
    "                + beta2*last_grad_params['s-K-l' + str(l)]\n",
    "            last_grad_params['s-b-l' + str(l)] = (1-beta2) * torch.square(dbl) \\\n",
    "                + beta2*last_grad_params['s-b-l' + str(l)]\n",
    "\n",
    "            model['K-l' + str(l)] += -alpha * torch.divide(\n",
    "                last_grad_params['v-K-l' + str(l)] / (1 - np.power(beta1, iteration)), \n",
    "                torch.sqrt(last_grad_params['s-K-l' + str(l)] / (1 - np.power(beta2, iteration))) \n",
    "                    + epsilon)\n",
    "            model['b-l' + str(l)] += -alpha * torch.divide(\n",
    "                last_grad_params['v-b-l' + str(l)] / (1 - np.power(beta1, iteration)), \n",
    "                torch.sqrt(last_grad_params['s-b-l' + str(l)] / (1 - np.power(beta2, iteration)))\n",
    "                    + epsilon)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown layer-type: {model_arch['t-l' + str(l)]}\")\n",
    "\n",
    "\n",
    "def optimize(X: torch.Tensor, Y: torch.Tensor, model: dict, model_arch: dict,\n",
    "        epochs: int = 1500, batch_size=None,\n",
    "        # debug tools\n",
    "        debug_mode: bool=False, cache_per_iter: int=100, \n",
    "        # updating the gradients\n",
    "        alpha: float = 0.0001, **kwargs):\n",
    "    \"\"\"Optimizes the weights and biases, i.e. trains them.\n",
    "\n",
    "    X: shape -> (m, h_a^{0}, w_a^{0}, c^{0})\n",
    "    Y: shape -> (m, h_a^{L}) ; here L is the final-layers' number\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    batch_size = m if not batch_size else batch_size\n",
    "\n",
    "    train_cache = list()\n",
    "    \n",
    "    print(f\"Gradient-descent... {{alpha: {alpha}, epochs: {epochs}}}\")\n",
    "    print(f\"................... {{batch-size: {batch_size}}}\")\n",
    "    print(f\"................... {{kwargs: {kwargs}}}\")\n",
    "    \n",
    "    iter_cost = None\n",
    "    iter_count = 0\n",
    "    rng = default_rng(2)\n",
    "    X_idx = np.arange(start=0, stop=m, step=1)\n",
    "    for e in tqdm(range(epochs)):\n",
    "        rng.shuffle(X_idx)\n",
    "        print(X.shape)\n",
    "        print(Y.shape)\n",
    "        X_shfl, Y_shfl = X[X_idx, :, :, :].reshape(X.shape), Y[X_idx, :].reshape(Y.shape)\n",
    "        for i in range(np.int32(np.ceil(m / batch_size))):\n",
    "            Xi, Yi = X_shfl[batch_size*i:batch_size*(i+1), :], \\\n",
    "                Y_shfl[batch_size*i:batch_size*(i+1), :]\n",
    "\n",
    "            Al, forward_cache = forward_propogate(Xi, model_arch, model, **kwargs)\n",
    "\n",
    "            backward_cache = gradients(Yi, forward_cache, model, model_arch, **kwargs)\n",
    "            update_model(model=model, forward_cache=forward_cache, \n",
    "                backward_cache=backward_cache, iteration=iter_count+1, \n",
    "                alpha=alpha, **kwargs)\n",
    "            \n",
    "            if iter_count % cache_per_iter == 0:\n",
    "                iter_cost = util.softmax_cost(Al=Al, Y=Yi, **kwargs)\n",
    "                if debug_mode:\n",
    "                    train_cache.append(((e,i, iter_count), iter_cost, \n",
    "                        forward_cache, backward_cache))\n",
    "                else:\n",
    "                    train_cache.append(((e, i, iter_count), iter_cost))\n",
    "\n",
    "            if torch.isnan(iter_cost):\n",
    "                return model, train_cache, f\"iter-(epoch, batch)-{(e,i)}... NaN-Abort!\"\n",
    "\n",
    "            iter_count += 1\n",
    "\n",
    "    return model, train_cache, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_classify(Al):\n",
    "    return torch.argmax(Al, axis=1)\n",
    "\n",
    "def measure_accuracy(X_train: torch.Tensor, X_test: torch.Tensor, \n",
    "        Y_train: torch.Tensor, Y_test: torch.Tensor, \n",
    "        model: dict, model_arch: dict):\n",
    "    m_train = X_train.shape[0]\n",
    "    Al, _ = forward_propogate(X_train, model_arch=model_arch, model=model)\n",
    "    train_acc = 100 * torch.sum(multiclass_classify(Al) \n",
    "            == torch.argmax(Y_train, axis=1)) / m_train\n",
    "    \n",
    "    m_test = X_test.shape[0]    \n",
    "    Al, _ = forward_propogate(X_test, model)\n",
    "    test_acc = 100 * torch.sum(multiclass_classify(Al) \n",
    "            == torch.argmax(Y_test, axis=0)) / m_test\n",
    "\n",
    "    print(f\"Train accuracy: {train_acc}%\")\n",
    "    print(f\"Test accuracy: {test_acc}%\")\n",
    "\n",
    "def model(model_arch: dict, **kwargs):\n",
    "    params = initialize(model_arch=model_arch)\n",
    "    params, train_cache, err_msg = optimize(\n",
    "        X=cu_train_X, Y=cu_train_Y, model=params, model_arch=model_arch, **kwargs)\n",
    "    if err_msg:\n",
    "        print(err_msg)\n",
    "    else:\n",
    "        measure_accuracy(X_train=cu_train_X, X_test=cu_test_X, \n",
    "            Y_train=cu_train_Y, Y_test=cu_test_Y, \n",
    "            model=params, model_arch=model_arch)\n",
    "\n",
    "        if train_cache is not None:\n",
    "            y = [cache[1].cpu().detach().numpy() for cache in train_cache]\n",
    "            x = [cache[0][2] for cache in train_cache]\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,10))\n",
    "            ax.plot(x, y)\n",
    "            ax.set_title(\"Cost-function vs Iteration\")\n",
    "            ax.set_ylabel(\"cost-function ($J$)\")\n",
    "            ax.set_xlabel(\"training-iteration ($i$)\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    return params, train_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # training-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arch = {\n",
    "    \"L\": 3,\n",
    "\n",
    "    \"a-l0\": cu_train_X.shape[1:], # set this equal to X.shape[1:]\n",
    "    \"c-l0\": cu_train_X.shape[-1], # set this equal to X.shape[-1]\n",
    "\n",
    "    \"t-l1\": \"conv\",\n",
    "    \"k-l1\": (4,4,3),\n",
    "    \"c-l1\": 8,\n",
    "    \"p-l1\": (-1,-1,0), # -1 demotes same-padding in that dimension\n",
    "    \"s-l1\": (1,1,1),\n",
    "    \"pl-t-l1\": \"max\",\n",
    "    \"pl-k-l1\": (8,8,1),\n",
    "    \"pl-p-l1\": (-1,-1,0),\n",
    "    \"pl-s-l1\": (8,8,1),\n",
    "    \"g-l1\": \"relu\",\n",
    "\n",
    "    \"t-l2\": \"conv\",\n",
    "    \"k-l2\": (2,2,3),\n",
    "    \"c-l2\": 16,\n",
    "    \"p-l2\": (-1,-1,0),\n",
    "    \"s-l2\": (1,1,1),\n",
    "    \"pl-t-l2\": \"max\",\n",
    "    \"pl-k-l2\": (4,4,1),\n",
    "    \"pl-p-l2\": (-1,-1,0),\n",
    "    \"pl-s-l2\": (4,4,1),\n",
    "    \"g-l2\": \"relu\",\n",
    "\n",
    "    \"t-l3\": \"flat\",\n",
    "    \"n-l3\": 6,\n",
    "    \"g-l3\": \"softmax\"\n",
    "}\n",
    "\n",
    "params, train_cache = model(model_arch=model_arch, \n",
    "        scaling_type='xavier', epochs=1500, batch_size=cu_train_X.shape[1], alpha=0.0001, \n",
    "        grad_scheme='batch',\n",
    "        debug_mode=False, cache_per_iter=100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80fe1bb2d351c7da6118ff9c1a62c9784868a6763a6756e3322751dc6fcdcdcc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
