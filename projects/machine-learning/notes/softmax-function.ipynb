{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax-function\n",
    "The Softmax function is a vector-valued function, and is defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "softmax(\\mathbf{x}) & = \n",
    "\\begin{bmatrix}\n",
    "\\frac{e^{x_1}}{\\sum_{i=1}^{n} e^{x_i}}\\\\\n",
    "\\frac{e^{x_2}}{\\sum_{i=1}^{n} e^{x_i}}\\\\ \n",
    "\\vdots\\\\\n",
    "\\frac{e^{x_n}}{\\sum_{i=1}^{n} e^{x_i}}\n",
    "\\end{bmatrix} \\\\\n",
    "\\newline\n",
    "\\text{where, } n & \\text{ is the dimensionality of the input-output vector space} \\\\\n",
    "\\mathbf{x} & \\text{ is the $(n,1)$-dimensional vector }[x_1, x_2, \\dots, x_n]^\\intercal\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The softmax function is also called the _soft-arg-max_ function. This is to emphasize its relation to a modified version of the conventional _arg-max_ function, which can be defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{arg\\,max}_{modified}(\\mathbf{x}) & = \\mathrm{one\\,hot}(\\mathrm{arg\\,max}(\\mathbf{x}), \\mathrm{dim}(\\mathbf{x})) \\\\\n",
    "& = [0, \\dots, 0, 1, 0, \\dots, 0]^\\intercal \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{where,}&\\qquad\\mathrm{arg\\,max}(\\mathbf{y}) && \\text{ returns the index of the largest element in the input-vector $\\mathbf{y}$} \\\\\n",
    "&\\qquad\\mathrm{one\\,hot}(x, n) && \\text{ returns an n-dimensional one-hot vector, with the $n^{th}$ element equal to 1} \\\\\n",
    "&\\qquad\\mathrm{arg\\,max}() && \\text{ is the conventional arg-max function}\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
